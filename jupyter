# -*- coding: utf-8 -*-
"""jupyter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vPFgn8HE2ttWepzgDLS7uAd0h_4XTpL8

# Tensorflow error debug
"""

from functools import reduce
import random
from multiprocessing import Pool
import tensorflow as tf
import swifter

import numpy as np
import pandas as pd
import tensorflow as tf
from functools import reduce
tf.enable_eager_execution()
from query_cnn import get_estimator
from collections import Counter
from config import config
from tqdm import tqdm

"""# generate test file"""

tquery = pd.read_csv("top100K_26JulyOnwards.csv")

from tensorflow import keras
from query_nn_keras_small_multi import get_keras_model
l3 = pd.read_csv("gs://gcp-ushi-search-platform-npe/ashwin/dev/cats.csv", header=None, names=["leafId"])
civm = pd.read_csv("./model_file/ivm-cat.csv")
clabel = 3026

pd.set_option('display.max_colwidth', -1)
l3c = pd.merge(l3, civm[["Departments_Depth", "leafId"]], on ="leafId", how="left").drop_duplicates()
l3c["Departments_Depth"] = [str(x).split(",")[-1] for x in l3c["Departments_Depth"]]
l3c = l3c.reset_index(drop=True)
l3c.head()

model = get_keras_model(None, label_cnt=clabel)

with open("./model_file/glove.42B.tokens.txt") as f:
    glove_token = f.read().split("\n")
    
tmapper = dict([(t, k) for k, t in enumerate(glove_token)])

import re
X = np.zeros((tquery.shape[0], 10))
with tqdm(enumerate(tquery["query"])) as iter_:
    for i, query in iter_:
        query = query.replace('\\', "").replace("/", " ").replace("'", "").replace('"', "")
        query = [x.strip().lower() for x in re.split(r"[\s-]", query) if x != ""]
        vec = []
        for k, q in enumerate(query[:10]):
            X[i, k] = tmapper.get(q, 0)

prediction = model.predict(X, batch_size=1024)

pred_index = np.where(prediction > 0.1)
pred_index = list(zip(*pred_index))
pred_index = [(a, b,
               prediction[a, b],
               tquery.loc[a, "query"],
               l3c.loc[b, "leafId"],
               l3c.loc[b, "Departments_Depth"]
               ) for a,b in pred_index]

pred_index = pd.DataFrame(pred_index, columns=["source", "target", "score", "query", "catid", "department"])

pd.set_option('display.max_colwidth', -1)
pred_index.head(20)

pred_index[["query", "department", "score"]].to_csv("qcs-test-file-glove.csv", index=False)

"""# Catalogue data"""

train_data = pd.read_csv("./model_file/train_data_multi_l4/part-00000-506780a4-0c06-49ba-9f37-0d10a5c3f2b3-c000.csv",
                        header=None, names=["query", "freq", "label"])

catalogue = pd.read_csv("gs://gcp-ushi-search-platform-multi-npe-data/relevancy_data/qcs/catalogue_training_data.csv")

import json
with open("./model_file/nvaluesMapping.json", "r") as f:
    nmap = json.load(f)
facet = [x for x in nmap if x["facetType"] == "DEPARTMENT"]
facet_map = dict([(x["value"], x["categoryId"]) for x in facet])

with open("./model_file/dep-catid.json", "w") as f:
    json.dump(facet_map, f)

import re
l3 = pd.read_csv("gs://gcp-ushi-search-platform-npe/ashwin/dev/cats.csv", header=None, names=["leafId"])

catalogue["leafID"] = [facet_map.get(x) for x in catalogue["normdep"]]
print(catalogue[catalogue.leafID.isna()].normdep.unique().shape)
catalogue = catalogue[~catalogue["leafID"].isna()].reset_index(drop=True)
print(catalogue.shape)

train_data["label"] = [[int(float(i.split(":")[0])) for i in x.split("|") if float(i.split(":")[-1]) > 0.1] 
                       for x in train_data["label"]]
l3list = l3["leafId"].to_list()
train_data["leafId"] = [ [l3list[y] for y in x] for x in train_data["label"]]

len(set([str(x) for x in l3list]) - set(catalogue.leafID.unique().tolist()))
len(set(catalogue.leafID.unique().tolist()) - set([str(x) for x in l3list]))

train_data["query"].unique().shape, train_data.shape

train_data = train_data.drop_duplicates(subset="query")
train_data = train_data[~train_data["query"].isna()].reset_index(drop=True)
train_data["query"].unique().shape, train_data.shape

train_data[[len(x) > 1 for x in train_data["label"]]].shape

leafs = list(set(catalogue.leafID.unique().tolist()) | set([str(x) for x in l3list]))
leafs = dict([(l, k) for k, l in enumerate(leafs)])
with open("./model_file/leafID-label.json", "w") as f:
    json.dump(leafs, f)

len(leafs)

train_data["type"] = "query"
catalogue = catalogue.rename(columns={"leafID": "leafId"})
catalogue["type"] = "catalogue"

train = pd.concat([train_data[["query", "leafId", "type"]], catalogue[["query", "leafId", "type"]]]).reset_index(drop=True)
train.loc[train["type"] == "catalogue", "leafId"] = [[x] for x in train.loc[train["type"] == "catalogue", "leafId"]]
train["labels"] = [[leafs[str(y)] for y in x] for x in train["leafId"]]

train["query"] = [" ".join(x.split()[:45]) for x in train["query"]]

train["target"] = [x[0] for x in train["labels"]]

train.shape

new_df = []
with tqdm(train[[len(x) > 1 for x in train["labels"]]].index) as iter_:
    for i in iter_:
        pload = df.loc[i, :].to_list()
        for x in df.loc[i, "labels"][1:]:
            pload = train.loc[i, :].to_list()
            pload[-1] = x
            new_df.append(pload)

len(new_df)

new_df = pd.DataFrame(new_df, columns=["query", "leafId", "type", "labels", "target"])
train = pd.concat([train, new_df]).reset_index(drop=True)

train = train.sort_values("query").reset_index(drop=True)

train.to_csv("gs://gcp-ushi-search-platform-multi-npe-data/relevancy_data/qcs/qcs_training_data.csv", index=False)
train.to_csv("./model_file/qcs_training_data.csv", index=False)

train.head()

tagg = train[["query", "target"]].groupby("target").count().reset_index(drop=False)

m, s = tagg["query"].describe()[["mean", "std"]]

tagg["norm"] = (tagg["query"] - m)/s
tagg["mm"] = np.log(tagg["query"]) / np.log(tagg["query"]).max()

tagg.sort_values("mm", ascending=True)[0:40]

"""# dataloader catalogue"""

df = train

df = df.reset_index()

target_map = df[["index", "target"]]
target_map = target_map.groupby("target").agg({"index": list})

target_map.head()

target_map.shape, len(lid)

from functools import reduce
import random
from multiprocessing import Pool

import numpy as np
import json
from query_nn_keras_small_multi import get_keras_model
import pandas as pd
import swifter
import tensorflow as tf
import tensorflow.compat.v1.keras.backend as K
#from tensorflow.keras.backend.tensorflow_backend import set_session

from tensorflow.keras.callbacks import TensorBoard, CSVLogger, ModelCheckpoint, EarlyStopping
from tensorflow.keras.utils import Sequence
from functools import reduce
# tf.enable_eager_execution()
from query_cnn import get_estimator
from collections import Counter
from config import config
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.models import load_model, save_model


with open("./model_file/leafID-label.json", "r") as f:
    lid = json.load(f)
    
# df["labels"] = [json.loads(y) for y in df["labels"]]
print(df.head())

# Split train/test set
# df = df.sample(frac=1).reset_index(drop=True)
dev_sample_index = -1 * int(0.1 * float(df.shape[0]))
df_, df_dev = df[:dev_sample_index].reset_index(drop=True), df[dev_sample_index:].reset_index(drop=True)
test_sample_index = -1 * int(0.01 * float(df_.shape[0]))
df_train, df_test = df_[:test_sample_index].reset_index(drop=True), df_[test_sample_index:].reset_index(drop=True)

class DataGenerator(Sequence):
    def __init__(self,
                 df,
                 batch_size,
                 cat_count,
                 dim=10,
                 cq_split=0.5,
                 train=True,
                 steps=None):
        self.batch_size = batch_size
        self.df = df
        self.steps = steps
        self.train = train
        self.dim = dim
        self.cq_split = cq_split
        self.cat_count = cat_count
        self.category = df.target.unique().tolist()
        
        print("category count", len(self.category), self.cat_count)
    
    def __len__(self):
        return self.steps or self.df.shape[0] // self.batch_size 
    
    def rnum(self, cnt, start, end, exclude=[]):
        iindex = []
        
        while len(iindex) < cnt:
            num = random.randint(start, end)
            if num not in iindex and num not in exclude:
                iindex.append(num)
        return iindex
        
    
    def get_cpi(self, category, num):
        
        temp = self.df[ self.df["target"] == category]
        
        ind1 = temp[(temp["type"] == "query") ].index.tolist()
        if len(ind1) >= num:
            ind1 = [ind1[ random.randint(0, len(ind1)-1) ] for x in range(num)]
            
        ind2 = temp[ (temp["type"] == "catalogue") ].index.tolist()
        if len(ind2) >= num:
            ind2 = [ind2[ random.randint(0, len(ind2)-1) ] for x in range(num)]
            
        if len(ind1) + len(ind2) != 2*num:
            ind = temp.index.tolist()
            ind = [ind[ random.randint(0, len(ind)-1) ] for x in range(num)]
        else:
            s1 = int(num * self.cq_split)
            s2 = num - s1
            ind = ind1[:s1] + ind2[:s2]
            
        assert len(ind) == num
        
        payload = []
        
        for ival in ind:
            vec = self.df.loc[ival, "X"].split()
#             vec = [random.randint(0, 100)] * 6
            vec = vec[:self.dim]
            vec = vec + [0]*(self.dim - len(vec))
            target = np.zeros((self.cat_count,))
            for yind in self.df.loc[ival, "labels"]:
                target[yind] = 1
            payload.append((vec, target))
            
        return payload
    
    def __getitem__(self, index):
        try:
            if self.train:
                num = pow(2, random.randint(2, 6))
                random.shuffle(self.category)
                categories = self.category[:int(self.batch_size//num)]
                payload = []
                for category in categories:
                    payload += self.get_cpi(category, num=num)
                random.shuffle(payload)

                anchor = np.array([np.array(i[0]) for i in payload])
                target = np.array([np.array(i[1]) for i in payload])
                del payload
                anchor = anchor.astype('float32')
                target = target.astype("float32")

                return anchor, target
            else:
                start = index * self.batch_size
                end = (index+1) * self.batch_size - 1
                if end > self.df.shape[0]-1:
                    start = 0
                    end = self.batch_size - 1
                anchor = self.df.loc[start:end, "X"].to_list()
                anchor = np.array([y[:self.dim] + [0]*(self.dim-len(y[:self.dim])) for y in anchor]).astype("float32")
                target = np.zeros((self.batch_size, self.cat_count))
                for k, x in enumerate(self.df.loc[start:end, "labels"]):
                    for xind in x:
                        target[k, xind] = 1
                target = target.astype("float32")
                return anchor, target
        except Exception as e:
            print(e)
            raise e
            return self[index]
        
    def on_epoch_end(self):
        if not self.train:
            self.df = self.df.sample(frac=1).reset_index(drop=True)

cat_count = len(lid)
print("cat_count :: ", cat_count)
train_dg = DataGenerator(df_train, batch_size=2048, cat_count=cat_count, train=True, steps=2000)
test_dg = DataGenerator(df_test, batch_size=2048, cat_count=cat_count, train=False)

x = train_dg[1]

x[0].shape

x[0][:10]

np.argmax(x[1][:10], axis=1)

random.randint(0, 0)

"""####  ----------------------------------------------------------------------------------------------------------"""

pow(2, random.randint(1, 5))

import re
tokens = []
with tqdm(train_data["query"]) as iter_:
    for x in iter_:
        tokens += [x.strip().lower().replace("'", "").replace('"', "") for x in re.split(r"[-\s]",
                                                                x.replace("/", " ")) if x != ""]
    
tokens = list(set(tokens))

with open("./model_file/glove.42B.tokens.txt") as f:
    glove_token = f.read().split("\n")

tmapper = dict([(t, k) for k, t in enumerate(glove_token)])

def vectorize(query):
    query = query.replace('\\', "").replace("/", " ").replace("'", "").replace('"', "")
    query = [x.strip().lower() for x in re.split(r"[\s-]", query) if x != ""]
    vec = []
    for q in query:
        vec.append(tmapper.get(q, 0))
    return " ".join([str(y) for y in vec])

train_data["X"] = train_data["query"].swifter.apply(lambda x: vectorize(x))

train_data.head()

train_data[[len(x) == 1 for x in train_data["label"]]].shape

len(glove_token)

df = train_data

df.to_csv("./model_file/multi_df.csv", index=False)

# Split train/test set
dev_sample_index = -1 * int(0.1 * float(df.shape[0]))
df_, df_dev = df[:dev_sample_index].reset_index(drop=True), df[dev_sample_index:].reset_index(drop=True)
test_sample_index = -1 * int(0.01 * float(df_.shape[0]))
df_train, df_test = df_[:test_sample_index].reset_index(drop=True), df_[test_sample_index:].reset_index(drop=True)

dfs = df_train
dfs = dfs.reset_index().rename(columns={"index": "cnt"})
dfs["X"] = [[int(y) for y in x.split()] for x in dfs["X"]]
dfs["combined"] = dfs[["cnt", "X"]].values.tolist()

"""# generate mapper file"""

from tqdm import tqdm

import swifter
def func(z):
    return [(x, z[0]) for x in [x for x in z[1] if x != 0]]
word_tuple = dfs["combined"].swifter.apply(func)

rwt = []
with tqdm(word_tuple) as iter_:
    for x in iter_:
        rwt += x
len(rwt)

mapper = pd.DataFrame(data=rwt, columns=["word", "query"])
mapper = mapper.groupby("word").agg({"query": list}).reset_index(drop=False)
mapper["qcount"] = [len(x) for x in mapper["query"]]
mapper = mapper.sort_values("qcount", ascending=False).reset_index(drop=True)
mapper.shape

mapper["wvalue"] =[glove_token[x] for x in  mapper["word"]]
mapper.qcount.describe()

mapper = mapper[mapper["qcount"] > 3]
mapper = mapper[mapper["qcount"] < 1e+6]
mapper = mapper.reset_index(drop=True)
mapper.shape

avail_index = [0]*df_train.shape[0]
for x in mapper["query"]:
    for y in x:
        avail_index[y] = 1
len([k for k, x in enumerate(avail_index) if x == 0])

mapper.to_csv("model_file/multi_mapper.csv", index=False)

"""# load mapper"""

with open("./model_file/glove.42B.tokens.txt") as f:
    glove_token = f.read().split("\n")
    
import json
    
l3 = pd.read_csv("./model_file/l3_deps.csv")

df = pd.read_csv("./model_file/multi_df.csv")
df["X"] = df["X"].fillna("").swifter.apply(lambda x: [int(y) for y in str(x).split()])

df["Y"] = [json.loads(y) for y in df["label"]]
print(df.head())

# Split train/test set
dev_sample_index = -1 * int(0.1 * float(df.shape[0]))
df_, df_dev = df[:dev_sample_index].reset_index(drop=True), df[dev_sample_index:].reset_index(drop=True)
test_sample_index = -1 * int(0.01 * float(df_.shape[0]))
df_train, df_test = df_[:test_sample_index].reset_index(drop=True), df_[test_sample_index:].reset_index(drop=True)

mapper = pd.read_csv("model_file/multi_mapper.csv")
mapper["query"] = [json.loads(x) for x in mapper["query"]]

from tensorflow.keras.utils import Sequence

class DataGenerator(Sequence):
    def __init__(self,
                 df,
                 batch_size,
                 cat_count,
                 train=True,
                 mapper=None,
                 steps=None):
        self.batch_size = batch_size
        self.df = df
        self.steps = steps
        self.train = train
        if mapper is not None:
            self.mapper = mapper
            self.mapper = self.mapper.set_index("word")
            self.words = self.mapper.index.to_list()
        self.category = list(range(cat_count))
        
        print("category count", len(self.category))
    
    def __len__(self):
        return self.steps or self.df.shape[0] // self.batch_size 
    
    def rnum(self, cnt, start, end, exclude=[]):
        iindex = []
        
        while len(iindex) < cnt:
            num = random.randint(start, end)
            if num not in iindex and num not in exclude:
                iindex.append(num)
        return iindex
        
    
    def get_cpi(self, word, num):
        ind = self.mapper.loc[word, "query"]
        ind = [ind[i] for i in [random.randint(0, len(ind)-1) for x in range(num)]]
        
        payload = []
        
        for ival in ind:
            vec = self.df.loc[ival, "X"][:10]
            vec = vec + [0]*(10 - len(vec))
#             assert word in vec
#             vec = np.array(vec)
            target = np.zeros((len(self.category),))
            for yind in self.df.loc[ival, "Y"]:
                target[yind] = 1
            payload.append((vec, target))
            
        return payload
    
    def __getitem__(self, index):
        try:
            if self.train:
                num = 32
                random.shuffle(self.words)
                words = self.words[:int(self.batch_size//num)]
                payload = []
                for word in words:
                    payload += self.get_cpi(word, num=num)
                random.shuffle(payload)
                anchor = np.array([np.array(i[0]) for i in payload])
                target = np.array([np.array(i[1]) for i in payload])
                return payload
                del payload
                anchor = anchor.astype('float32')
                target = target.astype("float32")

                return anchor, target
            else:
                start = index * self.batch_size
                end = (index+1) * self.batch_size - 1
                if end > self.df.shape[0]-1:
                    start = 0
                    end = self.batch_size - 1
                anchor = self.df.loc[start:end, "X"].to_list()
                anchor = np.array([y + [0]*(10-len(y)) for y in anchor]).astype("float32")
                target = np.zeros((self.batch_size, len(self.category)))
                for k, x in enumerate(self.df.loc[start:end, "Y"]):
                    for xind in x:
                        target[k, xind] = 1
                target = target.astype("float32")
                return anchor, target
        except Exception as e:
            print(e)
            raise e
            return self[index]
        
    def on_epoch_end(self):
        if not self.train:
            self.df = self.df.sample(frac=1).reset_index(drop=True)
            
cat_count = l3.shape[0]

train_dg = DataGenerator(df_train, batch_size=2048, cat_count=cat_count, train=True, mapper=mapper)
test_dg = DataGenerator(df_test, batch_size=2048, cat_count=cat_count, train=False, mapper=None)

x = train_dg[0]

np.array([x[0] for x in x]).shape

len([y for y in x[0] if len(y) > 10][0])

np.where(x[1][31:50] == 1)

"""# model training"""

# from query_nn_keras import get_keras_model

# cat_count = 3000

from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input,Embedding, Dense, LSTM, Conv2D, MaxPool2D, MaxPool1D, Bidirectional, TimeDistributed, GRU, Activation, Dropout
from tensorflow.keras.layers import multiply,Reshape,Flatten,Concatenate,RepeatVector, Permute
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import L1L2
import numpy as np

def acc(y_true, y_pred):
    targets = tf.argmax(y_true, 1)
    predictions = tf.argmax(y_pred, 1)
    correct_predictions = tf.equal(predictions, targets)
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"), name="accuracy")
    return accuracy
    
def acc_3(y_true, y_pred):
    targets = tf.argmax(y_true, 1)
    correct_predictions_top_3 = tf.math.in_top_k(y_pred, targets, 3, name=None)
    print(y_pred[0])
    accuracy_3 = tf.reduce_mean(tf.cast(correct_predictions_top_3,"float"), name="accuracy_3")
    return accuracy_3
    
def acc_5(y_true, y_pred):
    targets = tf.argmax(y_true, 1)
    correct_predictions_top_5 = tf.math.in_top_k(y_pred, targets, 5, name=None)
    accuracy_5 = tf.reduce_mean(tf.cast(correct_predictions_top_5,"float"), name="accuracy_5")
    return accuracy_5
 
def acc_9(y_true, y_pred):
    targets = tf.argmax(y_true, 1)
    correct_predictions_top_9 = tf.math.in_top_k(y_pred, targets, 9, name=None)
    accuracy_9 = tf.reduce_mean(tf.cast(correct_predictions_top_9,"float"), name="accuracy_9")
    return accuracy_9

def loss(y_true, y_pred):
    losses = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred, labels=y_true)
    loss = tf.reduce_mean(losses)
    return loss

def cnn_rnn_model(params):

    max_token_len = params['max_token_len']
    l2_reg_lambda = params['l2_reg_lambda']
    L2 = L1L2(l2=l2_reg_lambda)
    inputs = Input(shape=(max_token_len,), dtype='int32')

    # Embedding
    vocab_size = params['vocab_size']
    embedding_size = params['embedding_size']
    glove_vec = np.load("./model_file/glove_vec_wp.100d.npy")
    print(glove_vec.shape, vocab_size, embedding_size)
    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_size,
                          input_length=max_token_len, weights=[glove_vec], trainable=False)(inputs)

    x = Bidirectional (LSTM (256, return_sequences=True, dropout=0), merge_mode='concat')(embedding)
    x = Bidirectional (LSTM (256, return_sequences=True, dropout=0), merge_mode='concat')(x)
    x = TimeDistributed(Dense(512,activation='elu'))(x)
    x = Flatten()(x)
    x = Dense(4096,activation='elu')(x)
    
    num_classes = params['num_classes']
    output = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=inputs, outputs=output)
    adam = Adam(lr=params['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=params['epsilon'], decay=0.0)
    model.compile(optimizer=adam, loss=loss, metrics=[acc, acc_3, acc_5, acc_9])
    return model

def get_keras_model(freq,label_cnt=3954):
    params = {}
    params['learning_rate'] = 1e-2
    params['epsilon'] = 1e-8

    params['vocab_size'] = 1917497 # glove vocabulary size
    params['filter_sizes'] = [2,3,4] # related to ngrams
    params['num_filters'] = 128*2 # hidden units
    params['num_dense_units'] = [640*2,640*2,640, 320]
    params['embedding_size'] = 100
    params['num_classes'] = label_cnt # number of departments with varying levels
    params['attention_depth'] = 3
    params['attention_size'] = 128
    params['dropout_keep_prob'] = 0.7
    params['max_token_len'] = 10
    params['l2_reg_lambda'] = 10.0
    model = cnn_rnn_model(params)

    print(model.summary())
    return model

model = get_keras_model(None, cat_count)

history = model.fit_generator(
                    train_dg,
                    steps_per_epoch=500,
                    validation_data=test_dg,
                    validation_steps=10,
                    epochs=1,
                    max_queue_size=100,
                    workers=1,
                    use_multiprocessing=False,
                    initial_epoch=0,
                    callbacks=None)

tf.re

"""# generator"""

def map_to_index(x):
    xs, label_index = x
    for l in label_index:
        shuffle(label_index[l])
    for x in xs:
        shuffle(x)
    labels = reduce(lambda a,b: a+b, xs, [])
    mlabels = []
    mlabel_pointer = dict([(l, 0) for l in label_index])
    for l in labels:
        mlabels.append( label_index[l][ mlabel_pointer[l] % len(label_index[l]) ] )
        mlabel_pointer[l] += 1
    return mlabels

def input_fn(dfs, batch_size, training=True, pred=False):
    
    
    def generator():
        sample_per_case = 4
        cases = batch_size // sample_per_case
        number_of_batches = dfs.shape[0]//batch_size
        labels = word_tuple.index.tolist()

        label_index = {}
        for y in labels:
            val = word_tuple.loc[y, "indx"]
            shuffle(val)
            label_index[y] = val

        while True:
            if training:
                print("Creating new batch in train")
                labels_b = [np.random.choice(labels, size=cases, replace=False).tolist() * sample_per_case
                            for _ in range(number_of_batches)]

                perjob = list(range(0, len(labels_b), 2000))
                if len(perjob) == 0:
                    perjob.append(0)
                perjob.append(len(labels_b))
                perjob = list(zip(perjob[:-1], perjob[1:]))
                perjob = [(labels_b[z[0]: z[1]], label_index) for z in perjob]

                with Pool(14) as pool:
                    labels_b = pool.map(map_to_index, perjob)
                print("Done")
            else:
                print("Creating new batch in test")
                labels_b = [list(dfs.index)]

            for x in labels_b:
                for y in x:
                    yield ((dfs.loc[y, "X"], dfs.loc[y, "freq"]), dfs.loc[y, "Y"])

    datasets = tf.data.Dataset.from_generator(generator,
                                              ((tf.int64, tf.int64), tf.int64),
                                             ((tf.TensorShape([None]), tf.TensorShape([])), tf.TensorShape([])))
    datasets = datasets.batch(batch_size)
    return datasets

import tensorflow as tf
tf.enable_eager_execution()
import numpy as np

x = np.array([[1,0,1],
            [0,1,0]])

a,b = tf.math.top_k(x, k=2)

a.numpy()

b.numpy()

[x for x in tf.split(b, b.shape[0], axis=0)]

tf.argmax(x, axis=1).numpy()

import pandas as pd

l3 = pd.read_csv("gs://gcp-ushi-search-platform-npe/ashwin/dev/cats.csv", header=None, names=["leafId"])
civm = pd.read_csv("gs://gcp-ushi-search-platform-npe/ashwin/dev/ivm-cat.csv")
clabel = 3026

l3c = pd.merge(l3, civm[["Departments_Depth", "leafId"]], on ="leafId", how="left").drop_duplicates()
l3c["Departments_Depth"] = [str(x).split(",")[-1] for x in l3c["Departments_Depth"]]
l3c = l3c.reset_index(drop=True)

l3c.head()

x = pd.read_csv("./model_file/multi_df.csv")

x.head()

l3c.loc[140]

